{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tien Chau shoreline detection using `CoastSat`\n",
    "\n",
    "This web notebook is based on a template provided by K. Vos et al. (2019).\n",
    "The software `CoastSat` is described in details in the following publications: \n",
    "- Shoreline detection:                      https://doi.org/10.1016/j.envsoft.2019.104528\n",
    "- Accuracy assessment and applications:     https://doi.org/10.1016/j.coastaleng.2019.04.004\n",
    "- Beach slope estimation:                   https://doi.org/10.1029/2020GL088365\n",
    "\n",
    "It enables the users to extract time-series of shoreline change over the last 30+ years at their site of interest.\n",
    "There are four main steps:\n",
    "1. Retrieval of the satellite images of the region of interest from Google Earth Engine\n",
    "2. Shoreline extraction at sub-pixel resolution\n",
    "3. Intersection of the shorelines with cross-shore transects\n",
    "4. Tidal correction \n",
    "\n",
    "## Initial settings\n",
    "\n",
    "It is necessary to install the Anaconda system for Python, with necessary packages, including the Google Earth Engine Python API. \n",
    "\n",
    "Most likely you have to get a recent version of `matplotlibrc` file, by doing so: \n",
    "* navigate to the link https://raw.githubusercontent.com/matplotlib/matplotlib/v3.3.1/matplotlibrc.template\n",
    "* save the file to your disk with name `matplotlibrc` (without extension)\n",
    "* copy to the \"matplotlib library folder\", the exact folder is shown in the error message -- if any -- when you execute the code below (on my computer, the folder is `C:\\Users\\HP\\.julia\\conda\\3\\envs\\coastsat\\Lib\\site-packages\\matplotlib\\mpl-data`)\n",
    "\n",
    "Then the intialization is as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "plt.ion()\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from coastsat import SDS_download, SDS_preprocess, SDS_shoreline, SDS_tools, SDS_transects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retrieval of the images from GEE\n",
    "\n",
    "Define the region of interest (`polygon`), the date range (`dates`) and the satellite missions (`sat_list`) from which you wish to retrieve the satellite images. The images will be cropped on the Google Earth Engine server and only the region of interest will be downloaded as a .tif file. The files will stored in the directory defined in `filepath`. \n",
    "\n",
    "Make sure the area of your ROI is smaller than 100 km<sup>2</sup> (if larger, split it into smaller ROIs).\n",
    "\n",
    "The function `SDS_download.check_images_available(inputs)` will print the number of images available for your inputs. The Landsat images are divided into Tier 1 and Tier 2. Only Tier 1 images can be used for time-series analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images available between 1988-01-01 and 2020-09-01:\n",
      "- In Landsat Tier 1 & Sentinel-2 Level-1C:\n",
      "  S2: 264 images\n",
      "  Total: 264 images\n"
     ]
    }
   ],
   "source": [
    "# region of interest (longitude, latitude)\n",
    "polygon = [[[109.249, 13.3677],\n",
    "            [109.266, 13.3677],\n",
    "            [109.266, 13.3532],\n",
    "            [109.249, 13.3532],\n",
    "            [109.249, 13.3677]]]   \n",
    "\n",
    "dates = ['1988-01-01', '2020-09-01']    # date range\n",
    "sat_list = ['S2']   # satellite missions\n",
    "sitename = 'TIENCHAU'   # name of the site\n",
    "filepath = os.path.join(os.getcwd(), 'data')    # directory where the data will be stored\n",
    "\n",
    "# put all the inputs into a dictionary\n",
    "inputs = {'polygon': polygon, 'dates': dates, 'sat_list': sat_list,\n",
    "        'sitename': sitename, 'filepath':filepath}\n",
    "\n",
    "# before downloading the images, check how many images are available for your inputs\n",
    "SDS_download.check_images_available(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `SDS_download.retrieve_images(inputs)` retrives the satellite images from Google Earth Engine.\n",
    "\n",
    "By default, only Landsat Tier 1 Top-of-Atmosphere and Sentinel-2 Level-1C products are downloaded. \n",
    "\n",
    "In case you need to access Tier 2 images for qualitative analysis, you need to set `inputs['include_T2'] = True` before calling `retrieve_images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images available between 1988-01-01 and 2020-09-01:\n",
      "- In Landsat Tier 1 & Sentinel-2 Level-1C:\n",
      "  S2: 264 images\n",
      "  Total: 264 images\n",
      "\n",
      "Downloading images:\n",
      "S2: 264 images\n",
      "100%\n",
      "23 out of 264 Sentinel-2 images were merged (overlapping or duplicate)\n"
     ]
    }
   ],
   "source": [
    "# inputs['include_T2'] = True\n",
    "metadata = SDS_download.retrieve_images(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you have already retrieved the images**, just load the metadata file by only running the section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = SDS_download.get_metadata(inputs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shoreline extraction\n",
    "\n",
    "This section maps the position of the shoreline on the satellite images. The user can define the cloud threhold (`cloud_thresh`) and select the spatial reference system in which to output the coordinates of the mapped shorelines (`output_epsg`). See http://spatialreference.org/ to find the EPSG number corresponding to your local coordinate system. Make sure that your are using cartesian coordinates and not spherical coordinates (lat,lon) like WGS84. If unsure, use 3857 which is the web mercator projection (used by Google Maps).\n",
    "\n",
    "To quality control each shoreline detection and manually validate the mapped shorelines, the user has the option to set the parameter `check_detection` to **True**. To save a figure for each mapped shoreline set `save_figure` to **True**. \n",
    "\n",
    "The other parameters are for advanced users only and are described in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = { \n",
    "    # general parameters:\n",
    "    'cloud_thresh': 0.5,        # threshold on maximum cloud cover\n",
    "    'output_epsg': 3857,        # epsg code of spatial reference system desired for the output   \n",
    "    # quality control:\n",
    "    'check_detection': True,    # if True, shows each shoreline detection to the user for validation\n",
    "    'save_figure': True,        # if True, saves a figure showing the mapped shoreline for each image\n",
    "    # add the inputs defined previously\n",
    "    'inputs': inputs,\n",
    "    # [ONLY FOR ADVANCED USERS] shoreline detection parameters:\n",
    "    'min_beach_area': 4500,     # minimum area (in metres^2) for an object to be labelled as a beach\n",
    "    'buffer_size': 150,         # radius (in metres) of the buffer around sandy pixels considered in the shoreline detection\n",
    "    'min_length_sl': 200,       # minimum length (in metres) of shoreline perimeter to be valid\n",
    "    'cloud_mask_issue': False,  # switch this parameter to True if sand pixels are masked (in black) on many images  \n",
    "    'sand_color': 'default',    # 'default', 'dark' (for grey/black sand beaches) or 'bright' (for white sand beaches)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Save .jpg of the satellite images \n",
    "Saves .jpg files of the preprocessed satellite images (cloud masking + pansharpening/down-sampling) under *./data/sitename/jpeg_files\\preprocessed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satellite images saved as .jpg in D:\\NQChien\\CoastSat-master\\data\\TIENCHAU\\jpg_files\\preprocessed\n"
     ]
    }
   ],
   "source": [
    "SDS_preprocess.save_jpg(metadata, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Digitize a reference shoreline\n",
    "Creates a reference shoreline which helps to identify outliers and false detections. The reference shoreline is manually digitised by the user on one of the images. The parameter `max_dist_ref` defines the maximum distance from the reference shoreline (in metres) at which a valid detected shoreline can be. If you think that the default value of 100 m will not capture the full shoreline variability of your site, increase this value to an appropriate distance.\n",
    "\n",
    "For the case of Tien Chau it is necessary to add a reference shoreline because of the complex shape of the shoreline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference shoreline has been saved in D:\\NQChien\\CoastSat-master\\data\\TIENCHAU\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "settings['reference_shoreline'] = SDS_preprocess.get_reference_sl(metadata, settings)\n",
    "settings['max_dist_ref'] = 100  # max distance (in meters) allowed from the reference shoreline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch shoreline detection\n",
    "Extracts the 2D shorelines from the images in the spatial reference system specified by the user in `'output_epsg'`. The mapped shorelines are saved into `output.pkl` (under *./data/sitename*) and `output.geojson` (to be used in a GIS software).\n",
    "\n",
    "If you see that the sand pixels on the images are not being identified, change the parameter `sand_color` from `default` to `dark` or `bright` depending on the color of your beach. \n",
    "\n",
    "Most image data are available only from 2017 up to now. Many earlier images are covered with clouds, or the resolutions were not adequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping shorelines:\n",
      "S2:   100%\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "output = SDS_shoreline.extract_shorelines(metadata, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then remove duplicates and images with inaccurate georeferencing (threhsold at 10m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 duplicates\n",
      "1 bad georef\n"
     ]
    }
   ],
   "source": [
    "output = SDS_tools.remove_duplicates(output)  # removes duplicates (images taken on the same date by the same satellite)\n",
    "output = SDS_tools.remove_inaccurate_georef(output, 10)  # remove inaccurate georeferencing (set threshold to 10 m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For use in GIS applications, you can save the mapped shorelines as a GEOJSON layer which can be easily imported into QGIS for example. You can choose to save the shorelines as a collection of lines or points (sometimes the lines are crossing over so better to use points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "geomtype = 'lines'  # choose 'points' or 'lines' for the layer geometry\n",
    "gdf = SDS_tools.output_to_gdf(output, geomtype)\n",
    "gdf.crs = {'init':'epsg:'+str(settings['output_epsg'])}  # set layer projection\n",
    "# save GEOJSON layer to file\n",
    "gdf.to_file(os.path.join(inputs['filepath'], inputs['sitename'], '%s_output_%s.geojson'%(sitename,geomtype)),\n",
    "                                driver='GeoJSON', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copies of the output files `TIENCHAU_*.pkl` and `TIENCHAU_*.geojson` have been made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple plot of the mapped shorelines. The coordinates are stored in the output dictionnary together with the exact dates in UTC time, the georeferencing accuracy and the cloud cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15,8])\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Eastings')\n",
    "plt.ylabel('Northings')\n",
    "plt.grid(linestyle=':', color='0.5')\n",
    "for i in range(len(output['shorelines'])):\n",
    "    sl = output['shorelines'][i]\n",
    "    date = output['dates'][i]\n",
    "    plt.plot(sl[:,0], sl[:,1], '.', label=date.strftime('%d-%m-%Y'))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image has been saved to `All_shorelines.png` in `TIENCHAU`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Shoreline analysis\n",
    "\n",
    "In this section we show how to compute time-series of cross-shore distance along user-defined shore-normal transects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you have already mapped the shorelines**, just load the output file (`output.pkl`) by running the section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(inputs['filepath'], sitename)\n",
    "with open(os.path.join(filepath, sitename + '_output' + '.pkl'), 'rb') as f:\n",
    "    output = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 options to define the coordinates of the shore-normal transects:\n",
    "\n",
    "**Option 1**: (method of choice - but remember to save the transect locations) the user can interactively draw the shore-normal transects along the beach by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "transects = SDS_transects.draw_transects(output, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': array([[12162127.3485389 ,  1500992.00673831],\n",
      "       [12162208.58686935,  1501057.6223129 ]]), '2': array([[12162474.17371887,  1500720.17078645],\n",
      "       [12162517.91743526,  1500788.9109122 ]]), '3': array([[12162727.26236371,  1500610.81149546],\n",
      "       [12162749.13422191,  1500704.54803059]]), '4': array([[12162952.23004802,  1500551.44502322],\n",
      "       [12162967.85280387,  1500682.67617239]]), '5': array([[12163017.8456226 ,  1500532.69771619],\n",
      "       [12163242.81330691,  1500451.45938575]]), '6': array([[12163017.8456226 ,  1500517.07496034],\n",
      "       [12163005.34741792,  1500417.08932287]]), '7': array([[12162805.37614298,  1500554.56957439],\n",
      "       [12162824.12345001,  1500395.21746467]]), '8': array([[12162342.94256969,  1500623.30970015],\n",
      "       [12162277.32699511,  1500567.06777907]])}\n"
     ]
    }
   ],
   "source": [
    "print(transects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2**: the user can load the transect coordinates (make sure the spatial reference system is the same as defined previously by the parameter *output_epsg*) from a .geojson file by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_file = os.path.join(os.getcwd(), 'examples', 'NARRA_transects.geojson')\n",
    "transects = SDS_tools.transects_from_geojson(geojson_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 3**: manually provide the coordinates of the transects as shown in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transects = dict([])\n",
    "transects['NA1'] = np.array([[16843142, -3989358], [16843457, -3989535]])\n",
    "transects['NA2'] = np.array([[16842958, -3989834], [16843286, -3989983]])\n",
    "transects['NA3'] = np.array([[16842602, -3990878], [16842955, -3990949]])\n",
    "transects['NA4'] = np.array([[16842596, -3991929], [16842955, -3991895]])\n",
    "transects['NA5'] = np.array([[16842838, -3992900], [16843155, -3992727]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the location of the transects, make sure they are in the right location with the origin always landwards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15,8], tight_layout=True)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Eastings')\n",
    "plt.ylabel('Northings')\n",
    "plt.grid(linestyle=':', color='0.5')\n",
    "for i in range(len(output['shorelines'])):\n",
    "    sl = output['shorelines'][i]\n",
    "    date = output['dates'][i]\n",
    "    plt.plot(sl[:,0], sl[:,1], '.', label=date.strftime('%d-%m-%Y'))\n",
    "for i,key in enumerate(list(transects.keys())):\n",
    "    plt.plot(transects[key][0,0],transects[key][0,1], 'bo', ms=5)\n",
    "    plt.plot(transects[key][:,0],transects[key][:,1],'k-',lw=1)\n",
    "    plt.text(transects[key][0,0]-100, transects[key][0,1]+100, key,\n",
    "                va='center', ha='right', bbox=dict(boxstyle=\"square\", ec='k',fc='w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, intersect the transects with the 2D shorelines to obtain time-series of cross-shore distance.\n",
    "\n",
    "The time-series of shoreline change for each transect are saved in a .csv file in the data folder (all dates are in UTC time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-series of the shoreline change along the transects saved as:\n",
      "D:\\NQChien\\CoastSat-master\\data\\TIENCHAU\\transect_time_series.csv\n"
     ]
    }
   ],
   "source": [
    "# defines the along-shore distance over which to consider shoreline points to compute the median intersection (robust to outliers)\n",
    "settings['along_dist'] = 25 \n",
    "cross_distance = SDS_transects.compute_intersection(output, transects, settings) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the time-series of shoreline change along each transect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15,8], tight_layout=True)\n",
    "gs = gridspec.GridSpec(len(cross_distance),1)\n",
    "gs.update(left=0.05, right=0.95, bottom=0.05, top=0.95, hspace=0.05)\n",
    "for i,key in enumerate(cross_distance.keys()):\n",
    "    if np.all(np.isnan(cross_distance[key])):\n",
    "        continue\n",
    "    ax = fig.add_subplot(gs[i,0])\n",
    "    ax.grid(linestyle=':', color='0.5')\n",
    "    ax.set_ylim([-50,50])\n",
    "    ax.plot(output['dates'], cross_distance[key]- np.nanmedian(cross_distance[key]), '-o', ms=6, mfc='w')\n",
    "    ax.set_ylabel('distance [m]', fontsize=12)\n",
    "    ax.text(0.5,0.95, key, bbox=dict(boxstyle=\"square\", ec='k',fc='w'), ha='center',\n",
    "            va='top', transform=ax.transAxes, fontsize=14)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tidal correction\n",
    "\n",
    "This last section is meant to tidally-correct the time-series of shoreline change using time-series of tide level and an estimate of the beach slope.\n",
    "\n",
    "The estimated water levels for Phu Yen coast are retrieved from WxTide32 program and stored in the csv file `tide.csv`. In this file the times are in UTC, and the datum for the water levels is approx. Mean Sea Level.\n",
    "\n",
    "Based on the measured topography, we choose a typical beach slope to be 0.05 along all transects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dates', 'shorelines', 'filename', 'cloud_cover', 'geoaccuracy', 'idx', 'satname'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2 = output.copy()\n",
    "output2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2['dates'][0]\n",
    "o2 = {'dates':[output2['dates'][0]], \n",
    "      'shorelines':[output2['shorelines'][0]], \n",
    "      'filename':[output2['filename'][0]],\n",
    "      'cloud_cover':[output2['cloud_cover'][0]],\n",
    "      'geoaccuracy':[output2['geoaccuracy'][0]],\n",
    "      'idx':[output2['idx'][0]],\n",
    "      'satname':[output2['satname'][0]],\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output2['filename'][:5], output2['filename'][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Extracting closest points: 0%\r\n",
      "Extracting closest points: 1%\r\n",
      "Extracting closest points: 2%\r\n",
      "Extracting closest points: 3%\r\n",
      "Extracting closest points: 4%\r\n",
      "Extracting closest points: 5%\r\n",
      "Extracting closest points: 6%\r\n",
      "Extracting closest points: 7%\r\n",
      "Extracting closest points: 8%\r\n",
      "Extracting closest points: 9%\r\n",
      "Extracting closest points: 10%\r\n",
      "Extracting closest points: 11%\r\n",
      "Extracting closest points: 12%\r\n",
      "Extracting closest points: 13%\r\n",
      "Extracting closest points: 14%\r\n",
      "Extracting closest points: 15%\r\n",
      "Extracting closest points: 16%\r\n",
      "Extracting closest points: 17%\r\n",
      "Extracting closest points: 18%\r\n",
      "Extracting closest points: 19%\r\n",
      "Extracting closest points: 20%\r\n",
      "Extracting closest points: 21%\r\n",
      "Extracting closest points: 22%\r\n",
      "Extracting closest points: 23%\r\n",
      "Extracting closest points: 24%\r\n",
      "Extracting closest points: 25%\r\n",
      "Extracting closest points: 26%\r\n",
      "Extracting closest points: 27%\r\n",
      "Extracting closest points: 28%\r\n",
      "Extracting closest points: 29%\r\n",
      "Extracting closest points: 30%\r\n",
      "Extracting closest points: 31%\r\n",
      "Extracting closest points: 32%\r\n",
      "Extracting closest points: 33%\r\n",
      "Extracting closest points: 34%\r\n",
      "Extracting closest points: 35%\r\n",
      "Extracting closest points: 36%\r\n",
      "Extracting closest points: 37%\r\n",
      "Extracting closest points: 38%\r\n",
      "Extracting closest points: 39%\r\n",
      "Extracting closest points: 40%\r\n",
      "Extracting closest points: 41%\r\n",
      "Extracting closest points: 42%\r\n",
      "Extracting closest points: 43%\r\n",
      "Extracting closest points: 44%\r\n",
      "Extracting closest points: 45%\r\n",
      "Extracting closest points: 46%\r\n",
      "Extracting closest points: 47%\r\n",
      "Extracting closest points: 48%\r\n",
      "Extracting closest points: 49%\r\n",
      "Extracting closest points: 50%\r\n",
      "Extracting closest points: 50%\r\n",
      "Extracting closest points: 51%\r\n",
      "Extracting closest points: 52%\r\n",
      "Extracting closest points: 53%\r\n",
      "Extracting closest points: 54%\r\n",
      "Extracting closest points: 55%\r\n",
      "Extracting closest points: 56%\r\n",
      "Extracting closest points: 57%\r\n",
      "Extracting closest points: 58%\r\n",
      "Extracting closest points: 59%\r\n",
      "Extracting closest points: 60%\r\n",
      "Extracting closest points: 61%\r\n",
      "Extracting closest points: 62%\r\n",
      "Extracting closest points: 63%\r\n",
      "Extracting closest points: 64%\r\n",
      "Extracting closest points: 65%\r\n",
      "Extracting closest points: 66%\r\n",
      "Extracting closest points: 67%\r\n",
      "Extracting closest points: 68%\r\n",
      "Extracting closest points: 69%\r\n",
      "Extracting closest points: 70%\r\n",
      "Extracting closest points: 71%\r\n",
      "Extracting closest points: 72%\r\n",
      "Extracting closest points: 73%\r\n",
      "Extracting closest points: 74%\r\n",
      "Extracting closest points: 75%\r\n",
      "Extracting closest points: 76%\r\n",
      "Extracting closest points: 77%\r\n",
      "Extracting closest points: 78%\r\n",
      "Extracting closest points: 79%\r\n",
      "Extracting closest points: 80%\r\n",
      "Extracting closest points: 81%\r\n",
      "Extracting closest points: 82%\r\n",
      "Extracting closest points: 83%\r\n",
      "Extracting closest points: 84%\r\n",
      "Extracting closest points: 85%\r\n",
      "Extracting closest points: 86%\r\n",
      "Extracting closest points: 87%\r\n",
      "Extracting closest points: 88%\r\n",
      "Extracting closest points: 89%\r\n",
      "Extracting closest points: 90%\r\n",
      "Extracting closest points: 91%\r\n",
      "Extracting closest points: 92%\r\n",
      "Extracting closest points: 93%\r\n",
      "Extracting closest points: 94%\r\n",
      "Extracting closest points: 95%\r\n",
      "Extracting closest points: 96%\r\n",
      "Extracting closest points: 97%\r\n",
      "Extracting closest points: 98%\r\n",
      "Extracting closest points: 99%\r\n",
      "Extracting closest points: 100%"
     ]
    }
   ],
   "source": [
    "# load the measured tide data\n",
    "filepath = os.path.join(os.getcwd(),'data','TIENCHAU','TIENCHAU_tides.csv')\n",
    "tide_data = pd.read_csv(filepath, parse_dates=['dates'])\n",
    "dates_ts = [_.to_pydatetime() for _ in tide_data['dates']]\n",
    "tides_ts = np.array(tide_data['tide'])\n",
    "\n",
    "# get tide levels corresponding to the time of image acquisition\n",
    "dates_sat = output['dates']  #o2[...]\n",
    "tides_sat = SDS_tools.get_closest_datapoint(dates_sat, dates_ts, tides_ts)\n",
    "\n",
    "# plot the subsampled tide data\n",
    "fig, ax = plt.subplots(1,1,figsize=(15,4), tight_layout=True)\n",
    "ax.grid(which='major', linestyle=':', color='0.5')\n",
    "ax.plot(tide_data['dates'], tide_data['tide'], '-', color='0.6', label='all time-series')\n",
    "ax.plot(dates_sat, tides_sat, '-o', color='k', ms=6, mfc='w',lw=1, label='image acquisition')\n",
    "ax.set(ylabel='tide level [m]',xlim=[dates_sat[0],dates_sat[-1]], title='Water levels at the time of image acquisition');\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply tidal correction using a linear slope and a reference elevation to which project all the time-series of cross-shore change (to get time-series at Mean Sea Level, set `reference elevation` to 1.182 (the mean water level obtained from the time series). For the beach slope, a value of 0.05 is used for all transects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidal correction along each transect\n",
    "reference_elevation = 1.182 # = average of tidal time series, elevation at which you would like the shoreline time-series to be\n",
    "beach_slope = {'1':0.05, '2':0.05, '3':0.05, '4':0.05, '5':0.05, '6':0.05, '7':0.05, '8':0.05}\n",
    "cross_distance_tidally_corrected = {}\n",
    "for key in cross_distance.keys():\n",
    "    correction = (tides_sat - reference_elevation)/beach_slope[key]\n",
    "    cross_distance_tidally_corrected[key] = cross_distance[key] + correction\n",
    "    \n",
    "# store the tidally-corrected time-series in a .csv file\n",
    "out_dict = dict([])\n",
    "out_dict['dates'] = dates_sat\n",
    "for key in cross_distance_tidally_corrected.keys():\n",
    "    out_dict['Transect '+ key] = cross_distance_tidally_corrected[key]\n",
    "df = pd.DataFrame(out_dict)\n",
    "fn = os.path.join(settings['inputs']['filepath'],settings['inputs']['sitename'],\n",
    "                  'transect_time_series_tidally_corrected.csv')\n",
    "df.to_csv(fn, sep=',')\n",
    "print('Tidally-corrected time-series of the shoreline change along the transects saved as:\\n%s'%fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    }
   ],
   "source": [
    "# plot the time-series of shoreline change (both raw and tidally-corrected)\n",
    "fig = plt.figure(figsize=[8,5], tight_layout=True)\n",
    "# gs = gridspec.GridSpec(len(cross_distance),1)\n",
    "gs = gridspec.GridSpec(2,1) # for 2 subplots only \n",
    "# gs.update(left=0.05, right=0.95, bottom=0.05, top=0.95, hspace=0.05) # original\n",
    "gs.update(left=0.1, right=0.9, bottom=0.1, top=0.9, hspace=0.1)\n",
    "count = -1 # counting subplots\n",
    "for i,key in enumerate(cross_distance.keys()):\n",
    "    if np.all(np.isnan(cross_distance[key])):\n",
    "        continue\n",
    "    if key!='3' and key!='5':\n",
    "        continue  # only choose to plot for transects 3 and 5\n",
    "    count += 1\n",
    "    ax = fig.add_subplot(gs[count,0])\n",
    "    ax.grid(linestyle=':', color='0.5')\n",
    "    ax.set_ylim([-150,150])\n",
    "    ax.plot(output['dates'], cross_distance[key]- np.nanmedian(cross_distance[key]), '-o', ms=3, mfc='w', label='raw')\n",
    "    ax.plot(output['dates'], cross_distance_tidally_corrected[key]- np.nanmedian(cross_distance[key]), '-o', ms=3, mfc='w', label='tidally-corrected')\n",
    "    ax.set_ylabel('distance [m]', fontsize=12)\n",
    "    ax.text(0.5,0.95, 'Transect '+key, bbox=dict(boxstyle=\"square\", ec='k',fc='w'), ha='center',\n",
    "            va='center', transform=ax.transAxes, fontsize=8)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_dict['Transect 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['1', '2', '3', '4', '5', '6', '7', '8'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_distance.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "After tidal correction, the cross-shore distance is `cross_distance_tidally_corrected[key] - np.nanmedian(cross_distance[key]`. Of which, the transect with key `'5'` locates approximately at the tip of the sand spit. \n",
    "\n",
    "The length of the sand spit is approximately the alongshore distance from transect 3 to 5. \n",
    "\n",
    "The area of the sand spit is approximately bounded by the transects 3 and 7."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
